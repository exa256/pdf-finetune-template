{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up env.\n",
    "1. set up the Open AI API key\n",
    "2. move your PDF doc inside the ./data dir and set the path to the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # key\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "doc = \"\" # path to to doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pdf.base import PDFReader\n",
    "from llama_hub.file.unstructured.base import UnstructuredReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFReader()\n",
    "docs0 = loader.load_data(file=Path(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "title = \"Book Title\"\n",
    "metadata = {\"book_title\": title} # metadata get parsed into LLM at prompt time\n",
    "docs = [Document(text=doc_text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the python script has access to the PDF, run the below cell. You should see the stringified content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use different models and edit the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([])\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0.3),\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4-0613\", temperature=0.3), callback_manager=callback_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Dataset\n",
    "### Chunking PDF data\n",
    "this step will chunk your PDF into a list of chunked text. The total number of inference ran will be the number of nodes (depend on the length of the article) * number of questions per chunk\n",
    "\n",
    "### Synthetic data\n",
    "after the original text is chunked, we will create a set of query/response questions based on the chunk texts.\n",
    "Use either GPT3.5 or GPT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser.from_defaults()\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "print(nodes)\n",
    "print(len(nodes)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex\n",
    "import json\n",
    "\n",
    "num_questions_per_chunk = 6 # this will set the number of questios per chunk\n",
    "question_gen_query = (\n",
    "    \"You are an Expert Conversation Creator. Your task is to setup \"\n",
    "    f\"an examination and queries on the topic of {title}. Using the provided context, \"\n",
    "    f\"formulate {num_questions_per_chunk} queries that captures an important fact from the \"\n",
    "    \"context. \\n\"\n",
    "    \"You MUST obey the following criteria:\\n\"\n",
    "    \"- Restrict the question to the context information provided.\\n\"\n",
    "    \"- Query must be something a novice who is new and uninitiated to the topic will ask.\\n\"\n",
    "    \"- Do NOT create a question that cannot be answered from the context.\\n\"\n",
    "    \"- Phrase the question in a way that is easy for someone to also have asked, ie. searchable on Google\"\n",
    "    \"- Phrase the question so that it does NOT refer to specific context. \"\n",
    "    'For instance, do NOT put phrases like \"given provided context\" or \"in this work\" in the question, '\n",
    "    \"because if the question is asked elsewhere it wouldn't be provided specific context. Replace these terms \"\n",
    "    \"with specific details.\\n\"\n",
    "    \"BAD questions:\\n\"\n",
    "    \"What did the author do in his childhood\\n\"\n",
    "    \"What were the main findings in this report\\n\\n\"\n",
    "    \"GOOD questions:\\n\"\n",
    "    \"What did Barack Obama do in his childhood\\n\"\n",
    "    \"What were the main findings in the original Transformers paper by Vaswani et al.\\n\\n\"\n",
    "    \"Generate the questions below:\\n\"\n",
    ")\n",
    "\n",
    "fp = open(\"data/qa_pairs.jsonl\", \"w\")\n",
    "for idx, node in enumerate(nodes):\n",
    "    dataset_generator = DatasetGenerator(\n",
    "        [node],\n",
    "        question_gen_query=question_gen_query,\n",
    "        service_context=gpt_35_context,\n",
    "        metadata_mode=\"all\",\n",
    "    )\n",
    "    node_questions_0 = dataset_generator.generate_questions_from_nodes(num=10)\n",
    "    print(f\"[Node {idx}] Generated questions:\\n {node_questions_0}\")\n",
    "    # for each question, get a response\n",
    "    for question in node_questions_0:\n",
    "        index = SummaryIndex([node], service_context=gpt_35_context)\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(question)\n",
    "        out_dict = {\"query\": question, \"response\": str(response)}\n",
    "        print(f\"[Node {idx}] Outputs: {out_dict}\")\n",
    "        fp.write(json.dumps(out_dict) + \"\\n\")\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Synthetic Data to Open AI messages\n",
    "Open AI finetunning model will require data to follow the Open AI messaging format:\n",
    "[system_message, user_input, assistant_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"data/qa_pairs.jsonl\", \"r\")\n",
    "out_fp = open(\"data/qa_pairs_openai.jsonl\", \"w\")\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": f\"You are a helpful assistant to help me answer questions on the topic of {title}\",\n",
    "}\n",
    "for line in fp:\n",
    "    qa_pair = json.loads(line)\n",
    "    user_prompt = {\"role\": \"user\", \"content\": qa_pair[\"query\"]}\n",
    "    assistant_prompt = {\"role\": \"assistant\", \"content\": qa_pair[\"response\"]}\n",
    "    out_dict = {\n",
    "        \"messages\": [system_prompt, user_prompt, assistant_prompt],\n",
    "    }\n",
    "    out_fp.write(json.dumps(out_dict) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tunning New vs. Existing model\n",
    "the below param uses 3.5 Turbo for fine tunning, which is currently supported. Once you have an updated model ID from the following steps, you can replace this param with the model ID to train model further on new data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently OpenAI only supports 3.5 Turbo fine tunning. Once you have an updated model ID from the following steps, you can replace this param with the model ID to train model further\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\", \n",
    "    \"data/qa_pairs_openai.jsonl\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine.get_current_job()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Fine-tune\n",
    "once the job is done, you should receive an email update. You should be able to get the corresponding model ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
